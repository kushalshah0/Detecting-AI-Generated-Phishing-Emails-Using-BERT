{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4YYKaT3NYqL9kAyYH+jct",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushalshah0/Detecting-AI-Generated-Phishing-Emails-Using-BERT/blob/main/ai_generated_phishing_email_detection_FastAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhFEWqxz45jZ",
        "outputId": "5f909b08-34be-4818-82ac-f8e2d3f4b9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "!pip install fastapi uvicorn pyngrok transformers torch tensorflow pickle-mixin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLhW16zv6odD",
        "outputId": "6033ee48-e224-4433-cef3-5d781e9956fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.40.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting pickle-mixin\n",
            "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=5988 sha256=105804ba2e3184bb8ed1158dda336614b7614f958ee902805ff1fa164eb607a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/e2/5c/da8f96a08c63469bc8b10e206cd4c78e8886d8acb8699f84c2\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin, pyngrok\n",
            "Successfully installed pickle-mixin-1.0.2 pyngrok-7.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Verify model paths\n",
        "import os\n",
        "\n",
        "SAMPLE_NAME = \"sample1\"\n",
        "\n",
        "BASE_PATH = f\"/content/drive/MyDrive/Detect_AI_Phishing_Project/{SAMPLE_NAME}\"\n",
        "\n",
        "paths = {\n",
        "    \"LSTM\": f\"{BASE_PATH}/lstm_model.pt\",\n",
        "    \"GRU\": f\"{BASE_PATH}/gru_model.pt\",\n",
        "    \"BERT\": f\"{BASE_PATH}/bert/final_model\",\n",
        "    \"Tokenizer\": f\"{BASE_PATH}/rnn_tokenizer.pkl\"\n",
        "}\n",
        "\n",
        "for k, v in paths.items():\n",
        "    print(k, \"✅\" if os.path.exists(v) else \"❌\", v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6gU6DBw6rSJ",
        "outputId": "c4312826-ef48-4a67-e66f-997051b91fd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/lstm_model.pt\n",
            "GRU ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/gru_model.pt\n",
            "BERT ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/bert/final_model\n",
            "Tokenizer ✅ /content/drive/MyDrive/Detect_AI_Phishing_Project/sample1/rnn_tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create FastAPI project structure\n",
        "import os\n",
        "\n",
        "API_DIR = \"/content/api\"\n",
        "os.makedirs(API_DIR, exist_ok=True)\n",
        "\n",
        "files = [\"main.py\", \"models.py\", \"schemas.py\"]\n",
        "for f in files:\n",
        "    with open(os.path.join(API_DIR, f), \"w\") as fp:\n",
        "        fp.write(\"\")\n",
        "\n",
        "print(\"FastAPI files created:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaUNNnmV6vti",
        "outputId": "b1d1ee3d-976d-4b3d-f4e6-b928ff633467"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI files created: ['main.py', 'models.py', 'schemas.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write schemas.py\n",
        "%%writefile /content/api/schemas.py\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class EmailRequest(BaseModel):\n",
        "    text: str\n",
        "    model: str  # bert | lstm | gru\n",
        "\n",
        "class PredictionResponse(BaseModel):\n",
        "    model: str\n",
        "    prediction: str\n",
        "    confidence: float"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUp7M0FL69HX",
        "outputId": "dd4c1714-4092-4945-e9c7-50e704c31c00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/api/schemas.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fe90910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9132982e-7299-4459-c37e-ee41b064fab3"
      },
      "source": [
        "#@title Write models.py\n",
        "%%writefile /content/api/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "SAMPLE_NAME = \"sample1\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/Detect_AI_Phishing_Project/{SAMPLE_NAME}\"\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "# Define LSTM Model Architecture (assuming a basic setup)\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# Define GRU Model Architecture (assuming a basic setup)\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        _, hidden = self.gru(embedded)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# LOAD TOKENIZER\n",
        "with open(f\"{BASE_PATH}/rnn_tokenizer.pkl\", \"rb\") as f:\n",
        "    rnn_tokenizer = pickle.load(f)\n",
        "\n",
        "MAX_LEN = 200\n",
        "# Corrected model parameters based on checkpoint (from error message)\n",
        "MODEL_VOCAB_SIZE = 20000    # As indicated by embedding.weight shape in error\n",
        "EMBEDDING_DIM = 128         # As indicated by embedding.weight shape in error\n",
        "HIDDEN_DIM = 128            # As indicated by lstm.weight_ih_l0 shape in error\n",
        "OUTPUT_DIM = 1              # Binary classification\n",
        "\n",
        "# LOAD LSTM\n",
        "lstm_model = LSTMModel(MODEL_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)\n",
        "lstm_model.load_state_dict(torch.load(f\"{BASE_PATH}/lstm_model.pt\", map_location=DEVICE))\n",
        "lstm_model.eval()\n",
        "\n",
        "# LOAD GRU\n",
        "gru_model = GRUModel(MODEL_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)\n",
        "gru_model.load_state_dict(torch.load(f\"{BASE_PATH}/gru_model.pt\", map_location=DEVICE))\n",
        "gru_model.eval()\n",
        "\n",
        "# LOAD BERT\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "    f\"{BASE_PATH}/bert/final_model\"\n",
        ")\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    f\"{BASE_PATH}/bert/final_model\"\n",
        ").to(DEVICE)\n",
        "bert_model.eval()\n",
        "\n",
        "# HELPERS\n",
        "def preprocess_rnn(text):\n",
        "    seq = rnn_tokenizer.texts_to_sequences([text])\n",
        "    # Map token IDs >= MODEL_VOCAB_SIZE to 0 (assuming 0 is OOV/padding)\n",
        "    processed_seq = [[token_id if token_id < MODEL_VOCAB_SIZE else 0 for token_id in s] for s in seq]\n",
        "\n",
        "    padded = np.zeros((1, MAX_LEN))\n",
        "    # Ensure sequence is not longer than MAX_LEN\n",
        "    padded[0, :min(MAX_LEN, len(processed_seq[0]))] = processed_seq[0][:MAX_LEN]\n",
        "    return torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "def predict_rnn(model, text):\n",
        "    with torch.no_grad():\n",
        "        x = preprocess_rnn(text)\n",
        "        output = model(x)\n",
        "        prob = torch.sigmoid(output).item()\n",
        "        label = \"Phishing\" if prob >= 0.5 else \"Legitimate\"\n",
        "        confidence = prob if prob >= 0.5 else 1 - prob\n",
        "        return label, confidence\n",
        "\n",
        "def predict_bert(text):\n",
        "    inputs = bert_tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    ).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=1)\n",
        "        conf, pred = torch.max(probs, dim=1)\n",
        "        label = \"Phishing\" if pred.item() == 1 else \"Legitimate\"\n",
        "        return label, conf.item()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/api/models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Write main.py\n",
        "%%writefile /content/api/main.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from schemas import EmailRequest, PredictionResponse\n",
        "from models import predict_rnn, predict_bert, lstm_model, gru_model\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"AI-Generated Phishing Detection API\",\n",
        "    version=\"1.0\"\n",
        ")\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResponse)\n",
        "def predict(request: EmailRequest):\n",
        "    text = request.text\n",
        "    model_name = request.model.lower()\n",
        "\n",
        "    if model_name == \"bert\":\n",
        "        label, confidence = predict_bert(text)\n",
        "\n",
        "    elif model_name == \"lstm\":\n",
        "        label, confidence = predict_rnn(lstm_model, text)\n",
        "\n",
        "    elif model_name == \"gru\":\n",
        "        label, confidence = predict_rnn(gru_model, text)\n",
        "\n",
        "    else:\n",
        "        raise HTTPException(\n",
        "            status_code=400,\n",
        "            detail=\"Invalid model. Choose from: bert, lstm, gru\"\n",
        "        )\n",
        "\n",
        "    return PredictionResponse(\n",
        "        model=model_name,\n",
        "        prediction=label,\n",
        "        confidence=round(confidence, 4)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2emq9sJ-7Nig",
        "outputId": "1dd003ab-67ac-4173-859a-b23afe90f948"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/api/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b269a6c",
        "outputId": "dd0e6231-d51a-4811-8453-3e059152a45f"
      },
      "source": [
        "#@title Run FastAPI\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Kill any processes running on port 8000 or any uvicorn process\n",
        "!pkill -f uvicorn || true\n",
        "!fuser -k 8000/tcp || true\n",
        "\n",
        "%cd /content/api\n",
        "\n",
        "uvicorn_process = subprocess.Popen(\n",
        "    [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "print(\"FastAPI server starting...\")\n",
        "time.sleep(5)\n",
        "print(\"FastAPI server should be running.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "/content/api\n",
            "FastAPI server starting...\n",
            "FastAPI server should be running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "994520c3"
      },
      "source": [
        "#@title Display FastAPI Server Logs\n",
        "!cat nohup.out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adbb8f78",
        "outputId": "ad8cc8cf-be49-4b9d-9827-e37ab689ce88"
      },
      "source": [
        "#@title Test API Locally (with model selector)\n",
        "import requests\n",
        "\n",
        "API_URL = \"http://127.0.0.1:8000/predict\"\n",
        "\n",
        "email_text = \"Dear Customer,   Your bank account has been temporarily suspended due to suspicious activity.  Please click the link below to verify your account immediately:  http://verify-bank-login.com\" #@param {type:\"string\"}\n",
        "model_selector = \"bert\" #@param [\"bert\", \"lstm\", \"gru\"]\n",
        "\n",
        "payload = {\n",
        "    \"text\": email_text,\n",
        "    \"model\": model_selector\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, json=payload)\n",
        "\n",
        "print(\"Status Code:\", response.status_code)\n",
        "print(\"Response:\", response.json())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status Code: 200\n",
            "Response: {'model': 'bert', 'prediction': 'Phishing', 'confidence': 0.9971}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c66c97b",
        "outputId": "57a514b4-55e6-4958-a07b-e2c7449504e3"
      },
      "source": [
        "#@title Configure ngrok and Expose FastAPI\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"ngrok_auth_token\")\n",
        "print(\"ngrok authtoken set.\")\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "public_url_tunnel = ngrok.connect(8000)\n",
        "print(\"Public API URL:\", public_url_tunnel.public_url)\n",
        "print(\"Swagger Docs:\", public_url_tunnel.public_url + \"/docs\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken set.\n",
            "Public API URL: https://albert-unsheeting-lacteally.ngrok-free.dev\n",
            "Swagger Docs: https://albert-unsheeting-lacteally.ngrok-free.dev/docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5db0e024",
        "outputId": "c1ee8a46-7c65-488a-a518-c6ca88dae1c8"
      },
      "source": [
        "#@title Test API via ngrok URL\n",
        "import requests\n",
        "\n",
        "# Ensure public_url_tunnel is available from the previous cell execution\n",
        "if 'public_url_tunnel' not in locals():\n",
        "    print(\"Error: ngrok tunnel not established. Please run the 'Configure ngrok and Expose FastAPI' cell first.\")\n",
        "else:\n",
        "    NGROK_API_URL = public_url_tunnel.public_url + \"/predict\"\n",
        "\n",
        "    # Use the same input parameters as the local test\n",
        "    email_text = \"Dear Customer,   Your bank account has been temporarily suspended due to suspicious activity.  Please click the link below to verify your account immediately:  http://verify-bank-login.com\" #@param {type:\"string\"}\n",
        "    model_selector = \"bert\" #@param [\"bert\", \"lstm\", \"gru\"]\n",
        "\n",
        "    payload = {\n",
        "        \"text\": email_text,\n",
        "        \"model\": model_selector\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(NGROK_API_URL, json=payload)\n",
        "        print(\"Status Code:\", response.status_code)\n",
        "        print(\"Response:\", response.json())\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error connecting to ngrok URL: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status Code: 200\n",
            "Response: {'model': 'bert', 'prediction': 'Phishing', 'confidence': 0.9971}\n"
          ]
        }
      ]
    }
  ]
}